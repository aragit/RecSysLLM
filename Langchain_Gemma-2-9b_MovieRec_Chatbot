{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2532466,"sourceType":"datasetVersion","datasetId":1534693},{"sourceId":111293,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":93245,"modelId":117455}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install llama-cpp-python langchain faiss-cpu -q\n!pip install gradio  -q\n!pip install -U langchain-community accelerate bitsandbytes transformers sentence-transformers  -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T17:50:54.784750Z","iopub.execute_input":"2025-03-02T17:50:54.785090Z","iopub.status.idle":"2025-03-02T17:53:27.994545Z","shell.execute_reply.started":"2025-03-02T17:50:54.785060Z","shell.execute_reply":"2025-03-02T17:53:27.993268Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\nimport gradio as gr\nimport ast\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T17:53:47.051971Z","iopub.execute_input":"2025-03-02T17:53:47.052391Z","iopub.status.idle":"2025-03-02T17:53:53.177964Z","shell.execute_reply.started":"2025-03-02T17:53:47.052355Z","shell.execute_reply":"2025-03-02T17:53:53.177067Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Preprocessing","metadata":{"execution":{"iopub.status.busy":"2025-03-01T08:16:33.737511Z","iopub.execute_input":"2025-03-01T08:16:33.737920Z","iopub.status.idle":"2025-03-01T08:16:33.742894Z","shell.execute_reply.started":"2025-03-01T08:16:33.737884Z","shell.execute_reply":"2025-03-01T08:16:33.741529Z"}}},{"cell_type":"code","source":"\ndata = pd. read_csv('/kaggle/input/movie-recommendation-data/movies_metadata.csv')\nprint(\"Row data:\")\ndisplay(data.head().T)\n\n# Convert string representation of dictionaries to actual dictionaries\ndata['genres'] = data['genres'].apply(ast.literal_eval)\n\n# Transforming the 'genres' column\ndata['genres'] = data['genres'].apply(lambda x: [genre['name'] for genre in x])\n\n\n# Calculate weighted rate (IMDb formula)\ndef calculate_weighted_rate(vote_average, vote_count, min_vote_count=10):\n    return (vote_count / (vote_count + min_vote_count)) * vote_average + (min_vote_count / (vote_count + min_vote_count)) * 5.0\n\n# Minimum vote count to prevent skewed results\nvote_counts = data[data['vote_count'].notnull()]['vote_count'].astype('int')\nmin_vote_count = vote_counts.quantile(0.95)\n\n# Create a new column 'weighted_rate'\ndata['weighted_rate'] = data.apply(lambda row: calculate_weighted_rate(row['vote_average'], row['vote_count'], min_vote_count), axis=1)\ndata = data.dropna()\ndata = data[['genres', 'title', 'overview', 'weighted_rate']].reset_index(drop=True)\n\n\n# Create a new column by combining 'title', 'overview', and 'genre'\ndata['combined'] = data.apply(lambda row: f\"Title: {row['title']}. Overview: {row['overview']} Genres: {', '.join(row['genres'])}. Rating: {row['weighted_rate']}\", axis=1)\nprint(\"\"\"\nPrepared data:\"\"\")\ndisplay(data.head())\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Embeding","metadata":{}},{"cell_type":"code","source":"# Split text for embedding\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=512,\n    chunk_overlap=50\n)\n\n# Create texts AND metadatas together\ntexts = []\nmetadatas = []\nfor _, row in data.iterrows():\n    # Split text for this row\n    chunks = text_splitter.split_text(row['combined'])\n    # Create metadata for each chunk\n    for _ in chunks:\n        metadatas.append({\n            \"title\": row['title'],\n            \"overview\": row['overview']\n        })\n    texts.extend(chunks)\n\n# Now texts and metadatas have the same length\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n\n# Save FAISS index (optional, for reuse)\nvectorstore.save_local(\"movie_faiss_index\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load GGUF Model and Set up RAG Pipeline","metadata":{}},{"cell_type":"code","source":"import os\n# Load Gemma-2B GGUF\nllm = LlamaCpp(\n    model_path=\"/kaggle/input/gemma-2-9b-it/gguf/q4_k_m/1/gemma-2-9b-it-q4_k_m.gguf\",\n    temperature=0.2,       # Lower temp for more deterministic answers\n    max_tokens=256,        # Allow longer responses\n    n_ctx=2048,            # Increased context for better understanding\n    n_threads=os.cpu_count(),  # Fully utilize all CPU cores\n    n_batch=512,           # Optimized batch size for smoother inference\n    use_mlock=True,        # Lock model in RAM to prevent slow disk access\n    use_mmap=True,         # Improve performance by memory-mapping the model\n    verbose=False\n)\n\n# Custom prompt template\n\nprompt_template = \"\"\"\nYou are an expert movie recommender. For user queries about actors/directors/genres:\n1. Suggest 3 SPECIFIC movies with YEAR and LEAD ACTORS\n2. Include 1 to 3-sentence descriptions\n3. Explain WHY they match the request\n4. NEVER suggest irrelevant movies\n\nExample good response:\n\"Here are great Russell Crowe movies:\n- Gladiator (2000): A former Roman general seeks revenge on the corrupt emperor who murdered his family and sentenced him to slavery. Features Crowe's iconic performance.\n- A Beautiful Mind (2001): A Beautiful Mind is a 2001 American biographical drama film about the mathematician John Nash, a Nobel Laureate in Economics, played by Russell Crowe. Crowe won an Oscar for this role.\nWhy recommended? All showcase Crowe's range in historical dramas and character-driven stories.\"\n\nContext: {context}\nQuestion: {question}\nAnswer:\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"]\n)\n\n# Set up RetrievalQA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n    chain_type_kwargs={\"prompt\": PROMPT},\n    return_source_documents=True\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:00.999800Z","iopub.execute_input":"2025-03-02T14:48:01.000311Z","iopub.status.idle":"2025-03-02T14:49:17.089957Z","shell.execute_reply.started":"2025-03-02T14:48:01.000272Z","shell.execute_reply":"2025-03-02T14:49:17.088229Z"}},"outputs":[{"name":"stderr","text":"llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Build Gradio Interface","metadata":{}},{"cell_type":"code","source":"def handle_conversation(message, history):\n    # Cold-start handling\n    if not history:\n        return \"Welcome to MovieMaster! What kind of movies would you like to discover today?\"\n    \n    # Get recommendation\n    result = qa_chain({\"query\": message})\n    response = result[\"result\"]\n    \n    return response  # Return ONLY the LLM's response\n\n# Launch Gradio interface\ndemo = gr.ChatInterface(\n    fn=handle_conversation,\n    title=\"MovieMaster ğŸ¬\",\n    description=\"Your AI-powered movie recommendation assistant\",\n    examples=[\n        \"I like sci-fi movies with strong female leads\",\n        \"Recommend something similar to Inception\",\n        \"What are the best romantic movies from 1990s?\"\n    ], \n    theme=gr.themes.Soft()\n)\n\n\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install parameterized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T08:35:00.548247Z","iopub.execute_input":"2025-03-02T08:35:00.549067Z","iopub.status.idle":"2025-03-02T08:35:05.946478Z","shell.execute_reply.started":"2025-03-02T08:35:00.549024Z","shell.execute_reply":"2025-03-02T08:35:05.944220Z"}},"outputs":[{"name":"stdout","text":"Collecting parameterized\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nInstalling collected packages: parameterized\nSuccessfully installed parameterized-0.9.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import unittest\nimport time\nfrom parameterized import parameterized  # pip install parameterized\n\n# Replace with your actual `handle_conversation` and `qa_chain` objects\n# from your chatbot code\n# from your_chatbot_module import handle_conversation, qa_chain\n# Assuming qa_chain is defined and accessible. Example: qa_chain = RetrievalQA.from_chain_type(...)\n\nclass TestMovieChatbot(unittest.TestCase):\n\n    # Test cases organized into lists of (question, expected substring in response)\n    basic_recommendation_tests = [\n        (\"Recommend a good action movie.\", \"action\"),\n        (\"Suggest some comedies.\", \"comedy\"),\n        (\"What are some science fiction films?\", \"science fiction\"),\n        (\"Recommend a horror movie.\", \"horror\"),\n        (\"I want to watch a romantic movie.\", \"romantic\")\n    ]\n\n    actor_director_tests = [\n        (\"Movies with Tom Hanks.\", \"Tom Hanks\"),\n        (\"Recommend movies starring Leonardo DiCaprio.\", \"Leonardo DiCaprio\"),\n        (\"What movies did Christopher Nolan direct?\", \"Christopher Nolan\"),\n        (\"Suggest films directed by Quentin Tarantino.\", \"Quentin Tarantino\"),\n        (\"What movies star Meryl Streep?\", \"Meryl Streep\")\n    ]\n\n    combined_criteria_tests = [\n        (\"Action movies with Tom Cruise.\", \"Tom Cruise\"),\n        (\"Comedies directed by Woody Allen.\", \"Woody Allen\"),\n        (\"Science fiction films starring Sigourney Weaver.\", \"Sigourney Weaver\"),\n        (\"Romantic movies with Julia Roberts.\", \"Julia Roberts\"),\n        (\"Horror movies directed by John Carpenter.\", \"John Carpenter\")\n    ]\n\n    similar_to_tests = [\n        (\"Recommend something similar to Inception.\", \"Inception\"),\n        (\"What are some movies like The Matrix?\", \"The Matrix\"),\n        (\"Suggest movies similar to Pulp Fiction.\", \"Pulp Fiction\"),\n        (\"I want to watch something like Gladiator.\", \"Gladiator\"),\n        (\"What's a movie similar to Forrest Gump?\", \"Forrest Gump\")\n    ]\n\n    time_based_tests = [\n        (\"What are the best romantic movies from the 1990s?\", \"199\"),\n        (\"Recommend sci-fi films from the 1980s.\", \"198\"),\n        (\"Suggest a good comedy from 2010.\", \"2010\"),\n        (\"What were some great action movies from 2005?\", \"2005\"),\n        (\"Suggest horror movies from 1970s.\", \"197\")\n    ]\n\n    @parameterized.expand(basic_recommendation_tests)\n    def test_basic_recommendation(self, question, expected_substring):\n        result = qa_chain({\"query\": question})\n        self.assertIn(expected_substring, result[\"result\"])\n        self.assertGreater(len(result[\"result\"].split(\"\\n-\")), 0, \"Should suggest at least one movie.\")\n        # Add more assertions as needed (e.g., check for year, description)\n\n    @parameterized.expand(actor_director_tests)\n    def test_actor_director(self, question, expected_substring):\n        result = qa_chain({\"query\": question})\n        self.assertIn(expected_substring, result[\"result\"])\n        self.assertGreater(len(result[\"result\"].split(\"\\n-\")), 0, \"Should suggest at least one movie.\")\n\n    @parameterized.expand(combined_criteria_tests)\n    def test_combined_criteria(self, question, expected_substring):\n        result = qa_chain({\"query\": question})\n        self.assertIn(expected_substring, result[\"result\"])\n        self.assertGreater(len(result[\"result\"].split(\"\\n-\")), 0, \"Should suggest at least one movie.\")\n\n    @parameterized.expand(similar_to_tests)\n    def test_similar_to(self, question, expected_substring):\n        result = qa_chain({\"query\": question})\n        self.assertIn(expected_substring, result[\"result\"])\n        self.assertGreater(len(result[\"result\"].split(\"\\n-\")), 0, \"Should suggest at least one movie.\")\n\n    @parameterized.expand(time_based_tests)\n    def test_time_based(self, question, expected_substring):\n        result = qa_chain({\"query\": question})\n        self.assertIn(expected_substring, result[\"result\"])\n        self.assertGreater(len(result[\"result\"].split(\"\\n-\")), 0, \"Should suggest at least one movie.\")\n\n    def test_ambiguous_query(self):\n        # Check that it returns *something* without erroring. You'll need to analyze the response to ensure it's *reasonable*.\n        result = qa_chain({\"query\": \"Recommend a good movie.\"})\n        self.assertGreater(len(result[\"result\"]), 0) # Checks that the result is not empty\n\n    def test_negative_query(self):\n         # Should return a graceful message, *not* an error.\n        result = qa_chain({\"query\": \"Movies with Xyzzy Blorf\"})\n        self.assertIn(\"not found\", result[\"result\"].lower()) # Expect a \"not found\" style response\n\n    def test_cold_start(self):\n        response = handle_conversation(\"hello\", [])\n        self.assertIn(\"Welcome to MovieMaster\", response)\n\n    def test_performance(self):\n        start_time = time.time()\n        qa_chain({\"query\": \"Recommend a movie\"})\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        print(f\"Time taken for movie suggestion: {elapsed_time:.4f} seconds\")\n        self.assertLess(elapsed_time, 10) # Adjust the time limit as needed.  Depends on your hardware.\n\n# Example of how to run the tests from the command line\n# (if your tests are in a file named test_chatbot.py):\n# python -m unittest test_chatbot.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T08:40:07.730487Z","iopub.execute_input":"2025-03-02T08:40:07.731115Z","iopub.status.idle":"2025-03-02T08:40:07.764236Z","shell.execute_reply.started":"2025-03-02T08:40:07.731074Z","shell.execute_reply":"2025-03-02T08:40:07.762716Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Run tests BEFORE launching the interface\n    suite = unittest.TestLoader().loadTestsFromTestCase(TestMovieChatbot)\n    unittest.TextTestRunner(verbosity=2).run(suite)  # Verbosity=2 for more detailed output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T08:40:10.856600Z","iopub.execute_input":"2025-03-02T08:40:10.857006Z","execution_failed":"2025-03-02T08:45:12.765Z"}},"outputs":[{"name":"stderr","text":"test_actor_director_0_Movies_with_Tom_Hanks_ (__main__.TestMovieChatbot) ... ok\ntest_actor_director_1_Recommend_movies_starring_Leonardo_DiCaprio_ (__main__.TestMovieChatbot) ... FAIL\ntest_actor_director_2_What_movies_did_Christopher_Nolan_direct_ (__main__.TestMovieChatbot) ... ok\ntest_actor_director_3_Suggest_films_directed_by_Quentin_Tarantino_ (__main__.TestMovieChatbot) ... FAIL\ntest_actor_director_4_What_movies_star_Meryl_Streep_ (__main__.TestMovieChatbot) ... ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}