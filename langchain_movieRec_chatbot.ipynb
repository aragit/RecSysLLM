{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2532466,"sourceType":"datasetVersion","datasetId":1534693},{"sourceId":111293,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":93245,"modelId":117455}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install llama-cpp-python langchain faiss-cpu -q\n!pip install gradio  -q\n!pip install -U langchain-community accelerate bitsandbytes transformers sentence-transformers  -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T17:50:54.784750Z","iopub.execute_input":"2025-03-02T17:50:54.785090Z","iopub.status.idle":"2025-03-02T17:53:27.994545Z","shell.execute_reply.started":"2025-03-02T17:50:54.785060Z","shell.execute_reply":"2025-03-02T17:53:27.993268Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\nimport gradio as gr\nimport ast\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T17:53:47.051971Z","iopub.execute_input":"2025-03-02T17:53:47.052391Z","iopub.status.idle":"2025-03-02T17:53:53.177964Z","shell.execute_reply.started":"2025-03-02T17:53:47.052355Z","shell.execute_reply":"2025-03-02T17:53:53.177067Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Preprocessing","metadata":{"execution":{"iopub.status.busy":"2025-03-01T08:16:33.737511Z","iopub.execute_input":"2025-03-01T08:16:33.737920Z","iopub.status.idle":"2025-03-01T08:16:33.742894Z","shell.execute_reply.started":"2025-03-01T08:16:33.737884Z","shell.execute_reply":"2025-03-01T08:16:33.741529Z"}}},{"cell_type":"code","source":"\ndata = pd. read_csv('/kaggle/input/movie-recommendation-data/movies_metadata.csv')\nprint(\"Row data:\")\ndisplay(data.head().T)\n\n# Convert string representation of dictionaries to actual dictionaries\ndata['genres'] = data['genres'].apply(ast.literal_eval)\n\n# Transforming the 'genres' column\ndata['genres'] = data['genres'].apply(lambda x: [genre['name'] for genre in x])\n\n\n# Calculate weighted rate (IMDb formula)\ndef calculate_weighted_rate(vote_average, vote_count, min_vote_count=10):\n    return (vote_count / (vote_count + min_vote_count)) * vote_average + (min_vote_count / (vote_count + min_vote_count)) * 5.0\n\n# Minimum vote count to prevent skewed results\nvote_counts = data[data['vote_count'].notnull()]['vote_count'].astype('int')\nmin_vote_count = vote_counts.quantile(0.95)\n\n# Create a new column 'weighted_rate'\ndata['weighted_rate'] = data.apply(lambda row: calculate_weighted_rate(row['vote_average'], row['vote_count'], min_vote_count), axis=1)\ndata = data.dropna()\ndata = data[['genres', 'title', 'overview', 'weighted_rate']].reset_index(drop=True)\n\n\n# Create a new column by combining 'title', 'overview', and 'genre'\ndata['combined'] = data.apply(lambda row: f\"Title: {row['title']}. Overview: {row['overview']} Genres: {', '.join(row['genres'])}. Rating: {row['weighted_rate']}\", axis=1)\nprint(\"\"\"\nPrepared data:\"\"\")\ndisplay(data.head())\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Embeding","metadata":{}},{"cell_type":"code","source":"# Split text for embedding\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=512,\n    chunk_overlap=50\n)\n\n# Create texts AND metadatas together\ntexts = []\nmetadatas = []\nfor _, row in data.iterrows():\n    # Split text for this row\n    chunks = text_splitter.split_text(row['combined'])\n    # Create metadata for each chunk\n    for _ in chunks:\n        metadatas.append({\n            \"title\": row['title'],\n            \"overview\": row['overview']\n        })\n    texts.extend(chunks)\n\n# Now texts and metadatas have the same length\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n\n# Save FAISS index (optional, for reuse)\nvectorstore.save_local(\"movie_faiss_index\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load GGUF Model and Set up RAG Pipeline","metadata":{}},{"cell_type":"code","source":"import os\n# Load Gemma-2B GGUF\nllm = LlamaCpp(\n    model_path=\"/kaggle/input/gemma-2-9b-it/gguf/q4_k_m/1/gemma-2-9b-it-q4_k_m.gguf\",\n    temperature=0.2,       # Lower temp for more deterministic answers\n    max_tokens=256,        # Allow longer responses\n    n_ctx=2048,            # Increased context for better understanding\n    n_threads=os.cpu_count(),  # Fully utilize all CPU cores\n    n_batch=512,           # Optimized batch size for smoother inference\n    use_mlock=True,        # Lock model in RAM to prevent slow disk access\n    use_mmap=True,         # Improve performance by memory-mapping the model\n    verbose=False\n)\n\n# Custom prompt template\n\nprompt_template = \"\"\"\nYou are an expert movie recommender. For user queries about actors/directors/genres:\n1. Suggest 3 SPECIFIC movies with YEAR and LEAD ACTORS\n2. Include 1 to 3-sentence descriptions\n3. Explain WHY they match the request\n4. NEVER suggest irrelevant movies\n\nExample good response:\n\"Here are great Russell Crowe movies:\n- Gladiator (2000): A former Roman general seeks revenge on the corrupt emperor who murdered his family and sentenced him to slavery. Features Crowe's iconic performance.\n- A Beautiful Mind (2001): A Beautiful Mind is a 2001 American biographical drama film about the mathematician John Nash, a Nobel Laureate in Economics, played by Russell Crowe. Crowe won an Oscar for this role.\nWhy recommended? All showcase Crowe's range in historical dramas and character-driven stories.\"\n\nContext: {context}\nQuestion: {question}\nAnswer:\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"]\n)\n\n# Set up RetrievalQA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n    chain_type_kwargs={\"prompt\": PROMPT},\n    return_source_documents=True\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:00.999800Z","iopub.execute_input":"2025-03-02T14:48:01.000311Z","iopub.status.idle":"2025-03-02T14:49:17.089957Z","shell.execute_reply.started":"2025-03-02T14:48:01.000272Z","shell.execute_reply":"2025-03-02T14:49:17.088229Z"}},"outputs":[{"name":"stderr","text":"llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Build Gradio Interface","metadata":{}},{"cell_type":"code","source":"def handle_conversation(message, history):\n    # Cold-start handling\n    if not history:\n        return \"Welcome to MovieMaster! What kind of movies would you like to discover today?\"\n    \n    # Get recommendation\n    result = qa_chain({\"query\": message})\n    response = result[\"result\"]\n    \n    return response  # Return ONLY the LLM's response\n\n# Launch Gradio interface\ndemo = gr.ChatInterface(\n    fn=handle_conversation,\n    title=\"MovieMaster ğŸ¬\",\n    description=\"Your AI-powered movie recommendation assistant\",\n    examples=[\n        \"I like sci-fi movies with strong female leads\",\n        \"Recommend something similar to Inception\",\n        \"What are the best romantic movies from 1990s?\"\n    ], \n    theme=gr.themes.Soft()\n)\n\n\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}